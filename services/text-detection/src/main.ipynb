{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T01:18:43.525643Z",
     "iopub.status.busy": "2025-06-25T01:18:43.525127Z",
     "iopub.status.idle": "2025-06-25T01:18:43.553114Z",
     "shell.execute_reply": "2025-06-25T01:18:43.552589Z",
     "shell.execute_reply.started": "2025-06-25T01:18:43.525617Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T03:24:27.775004Z",
     "iopub.status.busy": "2025-06-25T03:24:27.773904Z",
     "iopub.status.idle": "2025-06-25T03:24:30.384885Z",
     "shell.execute_reply": "2025-06-25T03:24:30.383991Z",
     "shell.execute_reply.started": "2025-06-25T03:24:27.774957Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "# import kaggle\n",
    "# import opendatasets as od\n",
    "# import kagglehub\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "# from kagglehub import KaggleDatasetAdapter\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n"
     ]
    }
   ],
   "source": [
    "%env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl -L -o ./rustitw-russian-language-visual-text-recognition.zip\\\n",
    "  https://www.kaggle.com/api/v1/datasets/download/hardtype/rustitw-russian-language-visual-text-recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T03:24:30.386499Z",
     "iopub.status.busy": "2025-06-25T03:24:30.385995Z",
     "iopub.status.idle": "2025-06-25T03:24:30.390434Z",
     "shell.execute_reply": "2025-06-25T03:24:30.389710Z",
     "shell.execute_reply.started": "2025-06-25T03:24:30.386469Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "path_train = './rustitw-russian-language-visual-text-recognition/train/real/images/'\n",
    "path_test = './rustitw-russian-language-visual-text-recognition/test/real/images/'\n",
    "groundtruth_test_path = \"./test/groundtruth/\"\n",
    "output_train_easyocr_path = \"./train/easyocr_train_data/\"\n",
    "path_train_annotations= './rustitw-russian-language-visual-text-recognition/train/real/'\n",
    "path_test_annotations= './rustitw-russian-language-visual-text-recognition/test/real/'\n",
    "groundtruth_train_path = \"./train/groundtruth/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T03:24:38.961808Z",
     "iopub.status.busy": "2025-06-25T03:24:38.961171Z",
     "iopub.status.idle": "2025-06-25T03:24:38.968913Z",
     "shell.execute_reply": "2025-06-25T03:24:38.968218Z",
     "shell.execute_reply.started": "2025-06-25T03:24:38.961783Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def is_russian_text(text):\n",
    "    \"\"\"Checking the Russian text.\"\"\"\n",
    "    russian_pattern = re.compile(r'^[а-яА-ЯёЁ\\s\\n]+$')\n",
    "    return bool(russian_pattern.match(text))\n",
    "\n",
    "def prepare_annotations(info_csv_path, images_folder, max_samples=None):\n",
    "    \"\"\"Preparation of the annotation\"\"\"\n",
    "    data = pd.read_csv(info_csv_path)\n",
    "    annotations = []\n",
    "\n",
    "    if max_samples is not None:\n",
    "        data = data.head(max_samples)\n",
    "\n",
    "    for idx, row in data.iterrows():\n",
    "        image_path = images_folder + row['image_path']\n",
    "        img_width = row['width']\n",
    "        img_height = row['height']\n",
    "        bboxes = json.loads(row['box_and_label'])[0]\n",
    "\n",
    "        abs_bboxes = []\n",
    "        labels = []\n",
    "        for bbox in bboxes:\n",
    "    \n",
    "            label = bbox['label']\n",
    "            # if is_russian_text(label):\n",
    "            abs_bboxes.append(bbox)\n",
    "            labels.append(\"_\".join(label.split()))\n",
    "\n",
    "        if abs_bboxes:  \n",
    "            annotations.append({\n",
    "                'image_path': image_path,\n",
    "                'bboxes': abs_bboxes,\n",
    "                'labels': labels\n",
    "            })\n",
    "\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json_to_files(list_of_json, folder_path):\n",
    "    \"\"\"\n",
    "    Saves a list of JSON objects to individual files in a specified folder.\n",
    "\n",
    "    Args:\n",
    "        list_of_json (list): A list of JSON objects (as Python dictionaries).\n",
    "        folder_path (str): The path to the folder where files will be saved.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the output directory if it doesn't exist\n",
    "    try:\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "        print(f\"Directory '{folder_path}' created or already exists.\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error creating directory: {e}\")\n",
    "        return\n",
    "\n",
    "    for json_obj in list_of_json:\n",
    "        try:\n",
    "            # Extract the filename from the 'image_path' value\n",
    "            match = re.search(r'/(\\d+)\\.jpg$', json_obj.get(\"image_path\"))\n",
    "\n",
    "            if match:\n",
    "                extracted_string = match.group(1)\n",
    "            else:\n",
    "                print(\"No match was found.\")\n",
    "            file_name = os.path.basename(extracted_string+\".json\")\n",
    "\n",
    "            if not file_name:\n",
    "                print(\"Skipping JSON object due to missing 'image_path' or empty value.\")\n",
    "                continue\n",
    "\n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "            # Save the JSON object to the file\n",
    "            with open(file_path, 'w') as f:\n",
    "                json.dump(json_obj, f, indent=4)\n",
    "\n",
    "            print(f\"Successfully saved {file_path}\")\n",
    "\n",
    "        except KeyError:\n",
    "            print(\"Skipping a JSON object because it lacks the 'image_path' key.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sample(image_path, groundtruth_path):\n",
    "    \"\"\"Loads an image and its corresponding JSON annotation.\"\"\"\n",
    "    if not os.path.exists(image_path) or not os.path.exists(groundtruth_path):\n",
    "        return None, None\n",
    "        \n",
    "    # Load image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    width, height = image.size\n",
    "    # Load and parse groundtruth\n",
    "    with open(groundtruth_path, 'r', encoding='utf-8') as f:\n",
    "        gt_data = json.load(f)\n",
    "        \n",
    "    annotations = []\n",
    "    for item in gt_data['bboxes']:\n",
    "        lang = \"english\"\n",
    "        if is_russian_text(item['label']):\n",
    "            lang = \"russian\"\n",
    "\n",
    "        x = item['left'] * width\n",
    "        y = item['top'] * height\n",
    "        w = item['width'] * width\n",
    "        h = item['height'] * height\n",
    "        \n",
    "        left = x\n",
    "        top = y\n",
    "        right = x + w\n",
    "        bottom = y + h\n",
    "\n",
    "        annotations.append({\n",
    "            'text': item['label'],\n",
    "\n",
    "            'language': lang, # Handle cases where language might be missing\n",
    "            # The dataset uses [x1, y1, x2, y2, x3, y3, x4, y4] format\n",
    "            'points': [left, top,  right, top, right, bottom, left, bottom] \n",
    "        })\n",
    "        \n",
    "    return image, annotations\n",
    "    \n",
    "def get_all_filepaths(image_dir, gt_dir):\n",
    "    \"\"\"Returns a list of corresponding image and groundtruth file paths.\"\"\"\n",
    "    filepaths = []\n",
    "    for img_name in os.listdir(image_dir):\n",
    "        if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            base_name = os.path.splitext(img_name)[0]\n",
    "            gt_name = f\"{base_name}.json\"\n",
    "            \n",
    "            img_path = os.path.join(image_dir, img_name)\n",
    "            gt_path = os.path.join(gt_dir, gt_name)\n",
    "            if os.path.exists(gt_path):\n",
    "                filepaths.append((img_path, gt_path))\n",
    "    return filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T01:18:53.403417Z",
     "iopub.status.busy": "2025-06-25T01:18:53.402885Z",
     "iopub.status.idle": "2025-06-25T01:18:55.835445Z",
     "shell.execute_reply": "2025-06-25T01:18:55.834641Z",
     "shell.execute_reply.started": "2025-06-25T01:18:53.403394Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "real_train_annotations = prepare_annotations(path_train_annotations + 'info.csv', path_train_annotations)\n",
    "real_test_annotations = prepare_annotations(path_test_annotations + 'info.csv', path_test_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T18:37:11.161595Z",
     "iopub.status.busy": "2025-06-22T18:37:11.161056Z",
     "iopub.status.idle": "2025-06-22T18:37:11.166651Z",
     "shell.execute_reply": "2025-06-22T18:37:11.166060Z",
     "shell.execute_reply.started": "2025-06-22T18:37:11.161570Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24366"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(real_train_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-06-25T01:18:59.892088Z",
     "iopub.status.busy": "2025-06-25T01:18:59.891822Z",
     "iopub.status.idle": "2025-06-25T01:19:00.339900Z",
     "shell.execute_reply": "2025-06-25T01:19:00.339364Z",
     "shell.execute_reply.started": "2025-06-25T01:18:59.892065Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "save_json_to_files(real_test_annotations, groundtruth_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-06-25T01:19:04.137725Z",
     "iopub.status.busy": "2025-06-25T01:19:04.136948Z",
     "iopub.status.idle": "2025-06-25T01:19:06.840246Z",
     "shell.execute_reply": "2025-06-25T01:19:06.839487Z",
     "shell.execute_reply.started": "2025-06-25T01:19:04.137699Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "save_json_to_files(real_train_annotations, groundtruth_train_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T02:10:09.447324Z",
     "iopub.status.busy": "2025-06-25T02:10:09.447014Z",
     "iopub.status.idle": "2025-06-25T02:10:18.234506Z",
     "shell.execute_reply": "2025-06-25T02:10:18.233543Z",
     "shell.execute_reply.started": "2025-06-25T02:10:09.447279Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T03:24:53.874498Z",
     "iopub.status.busy": "2025-06-25T03:24:53.873913Z",
     "iopub.status.idle": "2025-06-25T03:24:54.156194Z",
     "shell.execute_reply": "2025-06-25T03:24:54.155447Z",
     "shell.execute_reply.started": "2025-06-25T03:24:53.874468Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "from shapely.geometry import Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T03:24:57.899428Z",
     "iopub.status.busy": "2025-06-25T03:24:57.898797Z",
     "iopub.status.idle": "2025-06-25T03:24:57.906719Z",
     "shell.execute_reply": "2025-06-25T03:24:57.905942Z",
     "shell.execute_reply.started": "2025-06-25T03:24:57.899400Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_reading_order_sort_key(box_data):\n",
    "    \"\"\"\n",
    "    Creates a sort key for top-to-bottom, left-to-right reading order.\n",
    "    Args:\n",
    "        box_data (dict): A dictionary with a 'points' key, which is a list\n",
    "                         of coordinates like [x1, y1, x2, y2, ...].\n",
    "    Returns:\n",
    "        A tuple (y, x) for sorting.\n",
    "    \"\"\"\n",
    "    points = np.array(box_data['points']).reshape(-1, 2)\n",
    "    # Use the y-coordinate of the top-left corner for primary sorting (top-to-bottom)\n",
    "    top_y = np.min(points[:, 1])\n",
    "    # Use the x-coordinate of the top-left corner for secondary sorting (left-to-right)\n",
    "    left_x = np.min(points[:, 0])\n",
    "    return (top_y, left_x)\n",
    "\n",
    "def calculate_image_cer(predictions, ground_truths):\n",
    "    \"\"\"\n",
    "    Calculates the Character Error Rate for the entire text content of an image.\n",
    "    \n",
    "    Args:\n",
    "        predictions (list of dict): Model's output, e.g., [{'text': 'hello', 'points': [...]}, ...].\n",
    "        ground_truths (list of dict): Ground truth annotations in the same format.\n",
    "        \n",
    "    Returns:\n",
    "        float: The calculated CER for the full image text.\n",
    "    \"\"\"\n",
    "    # --- Step 1: Sort both lists into reading order ---\n",
    "    sorted_predictions = sorted(predictions, key=get_reading_order_sort_key)\n",
    "    sorted_ground_truths = sorted(ground_truths, key=get_reading_order_sort_key)\n",
    "    \n",
    "    # --- Step 2: Concatenate text into single strings ---\n",
    "    # We use a space to separate words, which is a reasonable approximation.\n",
    "    predicted_text = \" \".join([p['text'].strip() for p in sorted_predictions])\n",
    "    ground_truth_text = \" \".join([gt['text'].strip() for gt in sorted_ground_truths])\n",
    "\n",
    "    # --- Step 3: Calculate Levenshtein distance and CER ---\n",
    "    if not ground_truth_text:\n",
    "        # If there is no ground truth text, CER is 0 if prediction is also empty,\n",
    "        # and 1 (100% error) otherwise.\n",
    "        return 0.0 if not predicted_text else 1.0\n",
    "\n",
    "    distance = levenshtein_distance(predicted_text.lower(), ground_truth_text.lower())\n",
    "    cer = distance / len(ground_truth_text)\n",
    "    \n",
    "    return cer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QWEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch, gc\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\n",
    "from transformers.generation import GenerationConfig\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T03:25:45.475971Z",
     "iopub.status.busy": "2025-06-25T03:25:45.475010Z",
     "iopub.status.idle": "2025-06-25T03:25:45.486364Z",
     "shell.execute_reply": "2025-06-25T03:25:45.485561Z",
     "shell.execute_reply.started": "2025-06-25T03:25:45.475941Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class QwenEvaluator:\n",
    "    def __init__(self, model_name=\"Qwen/Qwen-VL-Chat\"):\n",
    "        self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(model_name, \n",
    "                torch_dtype=torch.float16, \n",
    "                # attn_implementation=\"flash_attention_2\",\n",
    "                device_map=\"auto\")\n",
    "        self.processor = AutoProcessor.from_pretrained(model_name)\n",
    "    \n",
    "    def _parse_response(self, response_text):\n",
    "        \"\"\"Parses the model's string output into structured data.\"\"\"\n",
    "        predictions = []\n",
    "        pattern = r'\\{\\((\\d+),(\\d+),(\\d+),(\\d+)\\)\\}\\s*--\\s*(.*)'\n",
    "\n",
    "        # Use re.findall to get a list of all matching groups\n",
    "        matches = re.findall(pattern, response_text)\n",
    "        \n",
    "        for match in matches:\n",
    "            try:\n",
    "                # The first 4 elements are the coordinate strings\n",
    "                coords = match[:4]\n",
    "                # The 5th element is the text string\n",
    "                text = match[4]\n",
    "        \n",
    "                # Convert coordinate strings to integers\n",
    "                x1, y1, x2, y2 = map(int, coords)\n",
    "                \n",
    "                # Clean up the extracted text\n",
    "                cleaned_text = text.strip().replace('\"', '')\n",
    "                \n",
    "                # Convert to 4-point polygon format for your evaluator\n",
    "                # (top-left, top-right, bottom-right, bottom-left)\n",
    "                points = [x1, y1, x2, y1, x2, y2, x1, y2]\n",
    "                \n",
    "                predictions.append({'text': cleaned_text, 'points': points})\n",
    "            except (ValueError, IndexError) as e:\n",
    "                print(f\"Skipping a malformed match: {match} due to error: {e}\")\n",
    "        return predictions\n",
    "    def inference(self, image, image_path, prompt ='Read all the text in the image. For each section of text, print its bounding box and text in this bounding box in the format: {(x1,y1),(x2,y2)} -- text',\n",
    "                  sys_prompt=\"You are a helpful assistant.\", max_new_tokens=1024, return_input=False):\n",
    "        image_local_path = image_path\n",
    "        img_width, img_height = image.size\n",
    "        # prompt = 'Read all the text in the image. For each section of text, print its bounding box and text in this bounding box in the format: {(x1,y1),(x2,y2)} -- text'\n",
    "        # prompt = \"Read all the text in the image.\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": sys_prompt},\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\"image\": image_local_path},\n",
    "                ]\n",
    "            },\n",
    "        ]\n",
    "        text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        print(\"text:\", text)\n",
    "        # image_inputs, video_inputs = process_vision_info([messages])\n",
    "        inputs = self.processor(text=[text], images=[image], padding=True, return_tensors=\"pt\")\n",
    "        inputs = inputs.to('cuda')\n",
    "        with torch.no_grad():\n",
    "            output_ids =self.model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "        generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n",
    "        \n",
    "        response = self.processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "        print(response)\n",
    "        return self._parse_response(response[0])\n",
    "    def predict(self, image):\n",
    "        \"\"\"Performs OCR on a single image.\"\"\"\n",
    "        # img_width, img_height = image.size\n",
    "        \n",
    "        # Qwen-VL prompt for structured OCR\n",
    "        prompt = 'Detect all text in the image. For each text instance, provide its bounding box and content using the format <box>(x1,y1),(x2,y2)</box>text.'\n",
    "        \n",
    "        query = self.tokenizer.from_list_format([\n",
    "            {'image': image},\n",
    "            {'text': prompt},\n",
    "        ])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            response, _ = self.model.chat(self.tokenizer, query=query, history=None)\n",
    "            \n",
    "        return self._parse_response(response, img_width, img_height)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare for EasyOCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T03:25:52.741119Z",
     "iopub.status.busy": "2025-06-25T03:25:52.740584Z",
     "iopub.status.idle": "2025-06-25T03:25:52.745075Z",
     "shell.execute_reply": "2025-06-25T03:25:52.744498Z",
     "shell.execute_reply.started": "2025-06-25T03:25:52.741093Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T03:25:53.699051Z",
     "iopub.status.busy": "2025-06-25T03:25:53.698334Z",
     "iopub.status.idle": "2025-06-25T03:25:53.722968Z",
     "shell.execute_reply": "2025-06-25T03:25:53.722441Z",
     "shell.execute_reply.started": "2025-06-25T03:25:53.699028Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import shutil # For safely creating/deleting directories\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warp_and_crop(image: np.ndarray, box: np.ndarray):\n",
    "    \"\"\"\n",
    "    Takes a 4-point bounding box and warps it into a straightened, cropped image.\n",
    "    \"\"\"\n",
    "    rect = np.zeros((4, 2), dtype=\"float32\")\n",
    "    s = box.sum(axis=1); rect[0] = box[np.argmin(s)]; rect[2] = box[np.argmax(s)]\n",
    "    diff = np.diff(box, axis=1); rect[1] = box[np.argmin(diff)]; rect[3] = box[np.argmax(diff)]\n",
    "    (tl, tr, br, bl) = rect\n",
    "    \n",
    "    widthA = np.sqrt(((br[0] - bl[0])**2) + ((br[1] - bl[1])**2))\n",
    "    widthB = np.sqrt(((tr[0] - tl[0])**2) + ((tr[1] - tl[1])**2))\n",
    "    maxWidth = max(int(widthA), int(widthB))\n",
    "    \n",
    "    heightA = np.sqrt(((tr[0] - br[0])**2) + ((tr[1] - br[1])**2))\n",
    "    heightB = np.sqrt(((tl[0] - bl[0])**2) + ((tl[1] - bl[1])**2))\n",
    "    maxHeight = max(int(heightA), int(heightB))\n",
    "    \n",
    "    if maxWidth == 0 or maxHeight == 0: return None # Avoid errors on zero-size boxes\n",
    "    \n",
    "    dst = np.array([[0, 0], [maxWidth-1, 0], [maxWidth-1, maxHeight-1], [0, maxHeight-1]], dtype=\"float32\")\n",
    "    M = cv2.getPerspectiveTransform(rect, dst)\n",
    "    warped = cv2.warpPerspective(image, M, (maxWidth, maxHeight))\n",
    "    return warped\n",
    "\n",
    "# --- Part 2: Main Data Processing Function (combines logic from a.txt and b.txt) ---\n",
    "\n",
    "def create_train_val_split_and_process(base_image_dir, base_gt_dir, output_parent_dir, train_ratio=0.85, random_seed=42):\n",
    "    \"\"\"\n",
    "    Splits full images into train/val, processes them, and saves cropped words\n",
    "    into the specified folder structure.\n",
    "    \"\"\"\n",
    "    print(\"Starting data preparation...\")\n",
    "    \n",
    "    # --- Step 1: Define output paths and create directories ---\n",
    "    train_output_dir = os.path.join(output_parent_dir, 'en_train_filtered')\n",
    "    val_output_dir = os.path.join(output_parent_dir, 'en_val')\n",
    "    \n",
    "    train_results_dir = os.path.join(train_output_dir, '__results___files')\n",
    "    val_results_dir = os.path.join(val_output_dir, '__results___files')\n",
    "\n",
    "    # Clean and create directories\n",
    "    for d in [train_results_dir, val_results_dir]:\n",
    "        if os.path.exists(d):\n",
    "            shutil.rmtree(d) # Remove old data to ensure a clean slate\n",
    "        os.makedirs(d)\n",
    "        \n",
    "    print(f\"Created directories:\\n - {train_results_dir}\\n - {val_results_dir}\")\n",
    "\n",
    "    # --- Step 2: Get all file paths and split them into train and validation sets ---\n",
    "    all_filepaths = get_all_filepaths(base_image_dir, base_gt_dir)\n",
    "    \n",
    "    if not all_filepaths:\n",
    "        print(\"Error: No matching image and annotation files found.\")\n",
    "        return\n",
    "\n",
    "    train_files, val_files = train_test_split(\n",
    "        all_filepaths, \n",
    "        train_size=train_ratio, \n",
    "        random_state=random_seed\n",
    "    )\n",
    "    print(f\"Data split complete: {len(train_files)} training files, {len(val_files)} validation files.\")\n",
    "\n",
    "    # --- Step 3: Process each split (train and val) ---\n",
    "    \n",
    "    splits_to_process = {\n",
    "        \"train\": (train_files, train_results_dir),\n",
    "        \"validation\": (val_files, val_results_dir),\n",
    "    }\n",
    "\n",
    "    for mode, (file_list, results_dir) in splits_to_process.items():\n",
    "        print(f\"\\nProcessing {mode} data...\")\n",
    "        \n",
    "        label_data = [] # To store [filename, text] for the CSV\n",
    "\n",
    "        for img_path, gt_path in tqdm(file_list, desc=f\"Processing {mode} images\"):\n",
    "            image_cv = cv2.imread(img_path)\n",
    "            if image_cv is None: continue\n",
    "            \n",
    "            height, width, _ = image_cv.shape\n",
    "            if width * height > 933120000:  \n",
    "                continue\n",
    "            \n",
    "            if width > 65500 or height > 65500:  \n",
    "                continue\n",
    "              \n",
    "            with open(gt_path, 'r', encoding='utf-8') as f:\n",
    "                gt_data = json.load(f)\n",
    "            \n",
    "            # Assuming the annotation format is a list under the 'items' key\n",
    "            # and each item has 'points' and 'text'. Adjust if your format differs.\n",
    "            for i, item in enumerate(gt_data.get('items', gt_data.get('bboxes', []))):\n",
    "                x = item['left'] * width\n",
    "                y = item['top'] * height\n",
    "                w = item['width'] * width\n",
    "                h = item['height'] * height\n",
    "                left = x\n",
    "                top = y\n",
    "                right = x + w\n",
    "                bottom = y + h\n",
    "                \n",
    "                # --- FIX IS HERE ---\n",
    "                # Create the points array with the correct shape (4, 2)\n",
    "                points = np.array([\n",
    "                    [left, top],\n",
    "                    [right, top],\n",
    "                    [right, bottom],\n",
    "                    [left, bottom]\n",
    "                ], dtype=np.float32)\n",
    "                \n",
    "                if right <= left or bottom <= top:\n",
    "                    # print(f\"Некорректные координаты bounding box: left={left}, top={top}, right={right}, bottom={bottom}\")\n",
    "                    continue\n",
    "                if left < 0 or top < 0 or right > width or bottom > height:\n",
    "                    # print(f\"Bounding box выходит за пределы изображения: left={left}, top={top}, right={right}, bottom={bottom}\")\n",
    "                    continue \n",
    "                text = str(item.get('text', item.get('label', '')))\n",
    "\n",
    "                # --- Validation from a.txt ---\n",
    "                if not text: continue\n",
    "                \n",
    "                cropped_word_img = warp_and_crop(image_cv, points)\n",
    "                \n",
    "                if cropped_word_img is None or cropped_word_img.shape[0] == 0 or cropped_word_img.shape[1] == 0:\n",
    "                    continue\n",
    "\n",
    "                # --- Saving logic from b.txt ---\n",
    "                base_name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "                word_img_name = f\"{base_name}_word_{i}.png\"\n",
    "                \n",
    "                # Save the cropped word image to the correct '__results___files' folder\n",
    "                cv2.imwrite(os.path.join(results_dir, word_img_name), cropped_word_img)\n",
    "                \n",
    "                # Collect data for the CSV file\n",
    "                label_data.append([word_img_name, text])\n",
    "        \n",
    "        # --- Step 4: Write the labels.csv for the current split ---\n",
    "        if label_data:\n",
    "            df = pd.DataFrame(label_data, columns=['filename', 'words'])\n",
    "            output_label_path = os.path.join(results_dir, 'labels.csv')\n",
    "            df.to_csv(output_label_path, index=False, header=True, encoding='utf-8')\n",
    "            print(f\"Successfully created {len(label_data)} cropped images and labels.csv for {mode} set at:\\n {output_label_path}\")\n",
    "\n",
    "    print(\"\\nAll data processing and splitting complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T01:19:49.945091Z",
     "iopub.status.busy": "2025-06-25T01:19:49.944808Z",
     "iopub.status.idle": "2025-06-25T01:29:24.048230Z",
     "shell.execute_reply": "2025-06-25T01:29:24.047653Z",
     "shell.execute_reply.started": "2025-06-25T01:19:49.945072Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data preparation...\n",
      "Created directories:\n",
      " - ./all_data/en_train_filtered/__results___files\n",
      " - ./all_data/en_val/__results___files\n",
      "Data split complete: 20711 training files, 3655 validation files.\n",
      "\n",
      "Processing train data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train images: 100%|██████████| 20711/20711 [14:50<00:00, 23.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created 51429 cropped images and labels.csv for train set at:\n",
      " ./all_data/en_train_filtered/__results___files/labels.csv\n",
      "\n",
      "Processing validation data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing validation images: 100%|██████████| 3655/3655 [02:30<00:00, 24.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created 9080 cropped images and labels.csv for validation set at:\n",
      " ./all_data/en_val/__results___files/labels.csv\n",
      "\n",
      "All data processing and splitting complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "create_train_val_split_and_process(path_train, groundtruth_train_path, './all_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning EasyOCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rebuild EasyOCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T01:29:33.432682Z",
     "iopub.status.busy": "2025-06-25T01:29:33.432378Z",
     "iopub.status.idle": "2025-06-25T01:29:43.961778Z",
     "shell.execute_reply": "2025-06-25T01:29:43.960959Z",
     "shell.execute_reply.started": "2025-06-25T01:29:33.432661Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'EasyOCR'...\n",
      "remote: Enumerating objects: 2750, done.\u001b[K\n",
      "remote: Counting objects: 100% (661/661), done.\u001b[K\n",
      "remote: Compressing objects: 100% (86/86), done.\u001b[K\n",
      "remote: Total 2750 (delta 594), reused 575 (delta 575), pack-reused 2089 (from 1)\u001b[K\n",
      "Receiving objects: 100% (2750/2750), 157.82 MiB | 5.82 MiB/s, done.\n",
      "Resolving deltas: 100% (1689/1689), done.\n",
      "Updating files: 100% (313/313), done.\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/JaidedAI/EasyOCR.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T01:29:56.975735Z",
     "iopub.status.busy": "2025-06-25T01:29:56.975041Z",
     "iopub.status.idle": "2025-06-25T01:29:57.164322Z",
     "shell.execute_reply": "2025-06-25T01:29:57.163537Z",
     "shell.execute_reply.started": "2025-06-25T01:29:56.975705Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!cp -r ./EasyOCR/trainer ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T01:29:58.069984Z",
     "iopub.status.busy": "2025-06-25T01:29:58.069331Z",
     "iopub.status.idle": "2025-06-25T01:29:59.640370Z",
     "shell.execute_reply": "2025-06-25T01:29:59.639234Z",
     "shell.execute_reply.started": "2025-06-25T01:29:58.069957Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "files = os.listdir(\"./trainer\")\n",
    "for file in files:\n",
    "    !cp -r ./trainer/{file} ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T01:30:00.401879Z",
     "iopub.status.busy": "2025-06-25T01:30:00.401475Z",
     "iopub.status.idle": "2025-06-25T01:30:00.824230Z",
     "shell.execute_reply": "2025-06-25T01:30:00.823323Z",
     "shell.execute_reply.started": "2025-06-25T01:30:00.401842Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: invalid option -- 'y'\n",
      "Try 'rm --help' for more information.\n",
      "rm: invalid option -- 'y'\n",
      "Try 'rm --help' for more information.\n"
     ]
    }
   ],
   "source": [
    "!rm -r -y ./EasyOCR\n",
    "!rm -r -y ./trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-24T11:50:14.478062Z",
     "iopub.status.busy": "2025-06-24T11:50:14.477308Z",
     "iopub.status.idle": "2025-06-24T11:50:18.434136Z",
     "shell.execute_reply": "2025-06-24T11:50:18.433401Z",
     "shell.execute_reply.started": "2025-06-24T11:50:14.478035Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting gdown\n",
      "  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.15.4)\n",
      "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.7.4)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6 (from requests[socks]->gdown)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: PySocks, gdown\n",
      "Successfully installed PySocks-1.7.1 gdown-5.2.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T01:30:06.314804Z",
     "iopub.status.busy": "2025-06-25T01:30:06.314003Z",
     "iopub.status.idle": "2025-06-25T01:30:13.409746Z",
     "shell.execute_reply": "2025-06-25T01:30:13.409206Z",
     "shell.execute_reply.started": "2025-06-25T01:30:06.314770Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=15D8mC8gwLtkPqV53R_mTJTDPRQpQs-bX\n",
      "To: /app/proj/cyrillic_g2.pth\n",
      "100%|██████████| 15.3M/15.3M [00:00<00:00, 22.4MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cyrillic_g2.pth'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gdown; \n",
    "url = 'https://drive.google.com/uc?id=15D8mC8gwLtkPqV53R_mTJTDPRQpQs-bX' \n",
    "output = 'cyrillic_g2.pth' \n",
    "gdown.download(url, output, quiet=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write easyocr config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T04:35:33.178162Z",
     "iopub.status.busy": "2025-06-25T04:35:33.177386Z",
     "iopub.status.idle": "2025-06-25T04:35:33.181553Z",
     "shell.execute_reply": "2025-06-25T04:35:33.180810Z",
     "shell.execute_reply.started": "2025-06-25T04:35:33.178134Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T04:38:22.520380Z",
     "iopub.status.busy": "2025-06-25T04:38:22.519716Z",
     "iopub.status.idle": "2025-06-25T04:38:22.530351Z",
     "shell.execute_reply": "2025-06-25T04:38:22.529782Z",
     "shell.execute_reply.started": "2025-06-25T04:38:22.520343Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "config_yaml = {\n",
    "    'number': '0123456789',\n",
    "    'symbol': \"!\\\"#$%&'()*+,-./:;<=>?@[\\\\]№_`{|}~ €₽\",\n",
    "    'lang_char': 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyzАБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯабвгдеёжзийклмнопрстуфхцчшщъыьэюяЂђЃѓЄєІіЇїЈјЉљЊњЋћЌќЎўЏџҐґҒғҚқҮүҲҳҶҷӀӏӢӣӨөӮӯ',\n",
    "    # 'data_folder': '/kaggle/working/train/easyocr_train_data',\n",
    "    'train_data': 'all_data',\n",
    "    'valid_data': 'all_data/en_val',\n",
    "    # 'workers': 6,\n",
    "    # 'batch_size': 64,\n",
    "    'arch': 'VGG',\n",
    "    'saved_model': 'cyrillic_g2.pth',\n",
    "    # 'new_model':  '/kaggle/working/models/easyocr_finetuned/rustitw_recognizer.pth',\n",
    "    'select_data': 'train',\n",
    "    'experiment_name': 'easyocr_finetuned',\n",
    "    'batch_ratio': '1',\n",
    "    'total_data_usage_ratio':  1.0,\n",
    "    'batch_max_length': 2048,\n",
    "\n",
    "    # --- Other important parameters ---\n",
    "    'workers': 6,  # Start with a safe number like 2 for Kaggle\n",
    "    'batch_size': 64, # Start with a safe batch size\n",
    "    'imgH': 500,      # Standard height for OCR, do not use 65500\n",
    "    'imgW': 600,     # Standard width for OCR, it will be handled dynamically\n",
    "    \n",
    "    # 'imgH': 65500,\n",
    "    # 'imgW': 65500,\n",
    "    'rgb': False,\n",
    "    'FT': True,\n",
    "    'optim': False,\n",
    "    'lr': 0.0005,\n",
    "    'beta1': 0.9,\n",
    "    'rho': 0.95,\n",
    "    'eps': 0.00000001,\n",
    "    'grad_clip': 5,\n",
    "    'contrast_adjust': False,\n",
    "    'sensitive': True,\n",
    "    'PAD': True,\n",
    "    'contrast_adjust': 0.0,\n",
    "    'data_filtering_off': False,\n",
    "    'Transformation': 'None',\n",
    "    'FeatureExtraction': 'VGG',\n",
    "    'SequenceModeling': 'BiLSTM',\n",
    "    'Prediction': 'CTC',\n",
    "    'num_fiducial': 20,\n",
    "    'input_channel': 1,\n",
    "    'output_channel': 256,\n",
    "    'hidden_size': 256,\n",
    "    'decode': 'greedy',\n",
    "    'freeze_FeatureFxtraction': False,\n",
    "    'freeze_SequenceModeling': False,\n",
    "    'new_prediction': False,\n",
    "    'num_epoch': 5,\n",
    "    'num_iter':5000,\n",
    "    'valInterval': 500,\n",
    "    'lang_list': ['ru', 'en'],\n",
    "    'network_params' : {\n",
    "                 'input_channel': 1,\n",
    "                 'output_channel': 256,\n",
    "                 'hidden_size': 256\n",
    "                 },\n",
    "                 \n",
    "    'character_list': \"0123456789!\\#$%&'()*+,-./:;<=>?@[\\\\]№_`{|}~ €₽ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyzАБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯабвгдеёжзийклмнопрстуфхцчшщъыьэюяЂђЃѓЄєІіЇїЈјЉљЊњЋћЌќЎўЏџҐґҒғҚқҮүҲҳҶҷӀӏӢӣӨөӮӯ\"\n",
    "}\n",
    "with open('./config.yaml', 'w') as file:\n",
    "    yaml.dump(config_yaml, file, allow_unicode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### overwrite EasyOCR files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T01:30:42.368044Z",
     "iopub.status.busy": "2025-06-25T01:30:42.367488Z",
     "iopub.status.idle": "2025-06-25T01:30:42.402772Z",
     "shell.execute_reply": "2025-06-25T01:30:42.402126Z",
     "shell.execute_reply.started": "2025-06-25T01:30:42.368023Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./train.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import numpy as np\n",
    "\n",
    "# --- FIX: Explicit relative imports to avoid conflicts ---\n",
    "from utils import CTCLabelConverter, AttnLabelConverter, Averager\n",
    "from dataset import hierarchical_dataset, AlignCollate, Batch_Balanced_Dataset\n",
    "from model import Model\n",
    "from test import validation\n",
    "# --------------------------------------------------------\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def count_parameters(model):\n",
    "    # ... (this function is fine) ...\n",
    "    print(\"Modules, Parameters\")\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        total_params+=param\n",
    "        print(name, param)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "\n",
    "def train(opt, show_number = 2, amp=False):\n",
    "    # ... (dataset preparation is fine) ...\n",
    "    if not opt.data_filtering_off:\n",
    "        print('Filtering the images containing characters which are not in opt.character')\n",
    "        print('Filtering the images whose label is longer than opt.batch_max_length')\n",
    "\n",
    "    opt.select_data = opt.select_data.split('-')\n",
    "    opt.batch_ratio = opt.batch_ratio.split('-')\n",
    "    train_dataset = Batch_Balanced_Dataset(opt)\n",
    "    \n",
    "    log = open(f'./saved_models/{opt.experiment_name}/log_dataset.txt', 'a', encoding=\"utf8\")\n",
    "    AlignCollate_valid = AlignCollate(imgH=opt.imgH, imgW=opt.imgW, keep_ratio_with_pad=opt.PAD, contrast_adjust=opt.contrast_adjust)\n",
    "    valid_dataset, valid_dataset_log = hierarchical_dataset(root=opt.valid_data, opt=opt)\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=min(32, opt.batch_size),\n",
    "        shuffle=True,\n",
    "        num_workers=int(opt.workers),\n",
    "        collate_fn=AlignCollate_valid, pin_memory=True) # Removed prefetch_factor for broader compatibility\n",
    "    log.write(valid_dataset_log)\n",
    "    print('-' * 80)\n",
    "    log.write('-' * 80 + '\\n')\n",
    "    log.close()\n",
    "    \n",
    "    # ... (model configuration is fine) ...\n",
    "    if 'CTC' in opt.Prediction:\n",
    "        converter = CTCLabelConverter(opt.character)\n",
    "    else:\n",
    "        converter = AttnLabelConverter(opt.character)\n",
    "    opt.num_class = len(converter.character)\n",
    "\n",
    "    if opt.rgb:\n",
    "        opt.input_channel = 3\n",
    "    model = Model(opt)\n",
    "    print('model input parameters', opt.imgH, opt.imgW, opt.num_fiducial, opt.input_channel, opt.output_channel,\n",
    "          opt.hidden_size, opt.num_class, opt.batch_max_length, opt.Transformation, opt.FeatureExtraction,\n",
    "          opt.SequenceModeling, opt.Prediction)\n",
    "\n",
    "    if opt.saved_model != '':\n",
    "        print(f'loading pretrained model from {opt.saved_model}')\n",
    "        # Note: DataParallel is often not needed if device_map is used, but we'll keep it for compatibility with the script\n",
    "        model = torch.nn.DataParallel(model).to(device)\n",
    "        model.load_state_dict(torch.load(opt.saved_model, map_location=device), strict=False)\n",
    "    else:\n",
    "        # weight initialization\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'localization_fc2' in name:\n",
    "                print(f'Skip {name} as it is already initialized')\n",
    "                continue\n",
    "            try:\n",
    "                if 'bias' in name:\n",
    "                    init.constant_(param, 0.0)\n",
    "                elif 'weight' in name:\n",
    "                    init.kaiming_normal_(param)\n",
    "            except Exception as e:\n",
    "                if 'weight' in name:\n",
    "                    param.data.fill_(1)\n",
    "                continue\n",
    "        model = torch.nn.DataParallel(model).to(device)\n",
    "    \n",
    "    model.train() \n",
    "    print(\"Model:\")\n",
    "    # print(model) # Commented out to reduce log spam\n",
    "    count_parameters(model)\n",
    "    \n",
    "    # ... (setup loss is fine) ...\n",
    "    if 'CTC' in opt.Prediction:\n",
    "        criterion = torch.nn.CTCLoss(zero_infinity=True).to(device)\n",
    "    else:\n",
    "        criterion = torch.nn.CrossEntropyLoss(ignore_index=0).to(device)\n",
    "    loss_avg = Averager()\n",
    "    \n",
    "    # ... (optimizer setup is fine) ...\n",
    "    filtered_parameters = []\n",
    "    params_num = []\n",
    "    for p in filter(lambda p: p.requires_grad, model.parameters()):\n",
    "        filtered_parameters.append(p)\n",
    "        params_num.append(np.prod(p.size()))\n",
    "    print('Trainable params num : ', sum(params_num))\n",
    "    \n",
    "    if opt.optim=='adam':\n",
    "        optimizer = optim.Adam(filtered_parameters, lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "    else:\n",
    "        optimizer = optim.Adadelta(filtered_parameters, lr=opt.lr, rho=opt.rho, eps=opt.eps)\n",
    "    print(\"Optimizer:\")\n",
    "    print(optimizer)\n",
    "    \n",
    "    # ... (final options logging is fine) ...\n",
    "    with open(f'./saved_models/{opt.experiment_name}/opt.txt', 'a', encoding=\"utf8\") as opt_file:\n",
    "        opt_log = '------------ Options -------------\\n'\n",
    "        args = vars(opt)\n",
    "        for k, v in args.items():\n",
    "            opt_log += f'{str(k)}: {str(v)}\\n'\n",
    "        opt_log += '---------------------------------------\\n'\n",
    "        print(opt_log)\n",
    "        opt_file.write(opt_log)\n",
    "    \n",
    "    # ... (start training logic is fine) ...\n",
    "    start_iter = 0\n",
    "    if opt.saved_model != '':\n",
    "        try:\n",
    "            start_iter = int(opt.saved_model.split('_')[-1].split('.')[0])\n",
    "            print(f'continue to train, start_iter: {start_iter}')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    start_time = time.time()\n",
    "    best_accuracy = -1\n",
    "    best_norm_ED = -1\n",
    "    i = start_iter\n",
    "\n",
    "    # --- FIX 1: Correct GradScaler initialization ---\n",
    "    scaler = GradScaler()\n",
    "    # ----------------------------------------------\n",
    "    \n",
    "    t1 = time.time()\n",
    "        \n",
    "    while(True):\n",
    "        # ... (train part is mostly fine, but we'll ensure device placement) ...\n",
    "        image_tensors, labels = train_dataset.get_batch()\n",
    "        image = image_tensors.to(device)\n",
    "        text, length = converter.encode(labels, batch_max_length=opt.batch_max_length)\n",
    "        batch_size = image.size(0)\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        if amp:\n",
    "            # ... (amp logic) ...\n",
    "            pass\n",
    "        else:\n",
    "            if 'CTC' in opt.Prediction:\n",
    "                preds = model(image, text).log_softmax(2)\n",
    "                preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n",
    "                preds = preds.permute(1, 0, 2)\n",
    "                torch.backends.cudnn.enabled = False\n",
    "                # --- FIX 2: Ensure all tensors for loss are on the correct device ---\n",
    "                cost = criterion(preds, text.to(device), preds_size.to(device), length.to(device))\n",
    "                # --------------------------------------------------------------------\n",
    "                torch.backends.cudnn.enabled = True\n",
    "            else:\n",
    "                # ... (attention logic) ...\n",
    "                pass\n",
    "            cost.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), opt.grad_clip) \n",
    "            optimizer.step()\n",
    "        loss_avg.add(cost)\n",
    "\n",
    "        # validation part\n",
    "        if (i % opt.valInterval == 0) and (i!=0):\n",
    "            print('training time: ', time.time()-t1)\n",
    "            t1=time.time()\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f\"[{i}/{opt.num_iter}]... Running validation...\")\n",
    "            with open(f'./saved_models/{opt.experiment_name}/log_train.txt', 'a', encoding=\"utf8\") as log:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    # --- FIX 3: Pass the `device` object to the validation function ---\n",
    "                    valid_loss, current_accuracy, current_norm_ED, preds, confidence_score, labels,\\\n",
    "                    infer_time, length_of_data = validation(model, criterion, valid_loader, converter, opt, device)\n",
    "                    # -----------------------------------------------------------------\n",
    "                model.train()\n",
    "                loss_log = f'[{i}/{opt.num_iter}] Train loss: {loss_avg.val():0.5f}, Valid loss: {valid_loss:0.5f}, Elapsed_time: {elapsed_time:0.5f}'\n",
    "                loss_avg.reset()\n",
    "                current_model_log = f'{\"Current_accuracy\":17s}: {current_accuracy:0.3f}, {\"Current_norm_ED\":17s}: {current_norm_ED:0.4f}'\n",
    "                # keep best accuracy model (on valid dataset)\n",
    "                if current_accuracy > best_accuracy:\n",
    "                    best_accuracy = current_accuracy\n",
    "                    torch.save(model.state_dict(), f'./best_accuracy.pth')\n",
    "                if current_norm_ED > best_norm_ED:\n",
    "                    best_norm_ED = current_norm_ED\n",
    "                    torch.save(model.state_dict(), f'./best_norm_ED.pth')\n",
    "                best_model_log = f'{\"Best_accuracy\":17s}: {best_accuracy:0.3f}, {\"Best_norm_ED\":17s}: {best_norm_ED:0.4f}'\n",
    "\n",
    "                loss_model_log = f'{loss_log}\\n{current_model_log}\\n{best_model_log}'\n",
    "                print(loss_model_log)\n",
    "                log.write(loss_model_log + '\\n')\n",
    "\n",
    "                # show some predicted results\n",
    "                dashed_line = '-' * 80\n",
    "                head = f'{\"Ground Truth\":25s} | {\"Prediction\":25s} | Confidence Score & T/F'\n",
    "                predicted_result_log = f'{dashed_line}\\n{head}\\n{dashed_line}\\n'\n",
    "                \n",
    "                #show_number = min(show_number, len(labels))\n",
    "                \n",
    "                start = random.randint(0,len(labels) - show_number )    \n",
    "                for gt, pred, confidence in zip(labels[start:start+show_number], preds[start:start+show_number], confidence_score[start:start+show_number]):\n",
    "                    if 'Attn' in opt.Prediction:\n",
    "                        gt = gt[:gt.find('[s]')]\n",
    "                        pred = pred[:pred.find('[s]')]\n",
    "\n",
    "                    predicted_result_log += f'{gt:25s} | {pred:25s} | {confidence:0.4f}\\t{str(pred == gt)}\\n'\n",
    "                predicted_result_log += f'{dashed_line}'\n",
    "                print(predicted_result_log)\n",
    "                log.write(predicted_result_log + '\\n')\n",
    "                print('validation time: ', time.time()-t1)\n",
    "                t1=time.time()\n",
    "        # save model per 1e+4 iter.\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            torch.save(\n",
    "                model.state_dict(), f'./saved_models/{opt.experiment_name}/iter_{i+1}.pth')\n",
    "        \n",
    "        if i == opt.num_iter:\n",
    "            print('end the training')\n",
    "            break\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T01:30:47.838425Z",
     "iopub.status.busy": "2025-06-25T01:30:47.838125Z",
     "iopub.status.idle": "2025-06-25T01:30:47.869988Z",
     "shell.execute_reply": "2025-06-25T01:30:47.869347Z",
     "shell.execute_reply.started": "2025-06-25T01:30:47.838401Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./test.py\n",
    "\n",
    "import os\n",
    "import time\n",
    "import string\n",
    "import argparse\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from nltk.metrics.distance import edit_distance\n",
    "\n",
    "# --- FIX: Explicit relative imports ---\n",
    "from utils import CTCLabelConverter, AttnLabelConverter, Averager\n",
    "from dataset import hierarchical_dataset, AlignCollate\n",
    "from model import Model\n",
    "\n",
    "def validation(model, criterion, evaluation_loader, converter, opt, device): # <-- FIX: Accept `device`\n",
    "    \"\"\" validation or evaluation \"\"\"\n",
    "    n_correct = 0\n",
    "    norm_ED = 0\n",
    "    length_of_data = 0\n",
    "    infer_time = 0\n",
    "    valid_loss_avg = Averager()\n",
    "\n",
    "    for i, (image_tensors, labels) in enumerate(evaluation_loader):\n",
    "        batch_size = image_tensors.size(0)\n",
    "        length_of_data = length_of_data + batch_size\n",
    "        \n",
    "        # --- FIX: Move image tensors to the correct device ---\n",
    "        image = image_tensors.to(device)\n",
    "        # -----------------------------------------------------\n",
    "\n",
    "        # For max length prediction\n",
    "        length_for_pred = torch.IntTensor([opt.batch_max_length] * batch_size).to(device)\n",
    "        text_for_pred = torch.LongTensor(batch_size, opt.batch_max_length + 1).fill_(0).to(device)\n",
    "\n",
    "        text_for_loss, length_for_loss = converter.encode(labels, batch_max_length=opt.batch_max_length)\n",
    "\n",
    "        start_time = time.time()\n",
    "        if 'CTC' in opt.Prediction:\n",
    "            preds = model(image, text_for_pred).log_softmax(2)\n",
    "            forward_time = time.time() - start_time\n",
    "\n",
    "            # Calculate evaluation loss for CTC deocder.\n",
    "            preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n",
    "            # permute 'preds' to use CTCloss format\n",
    "            # --- FIX: Ensure all tensors for loss are on the correct device ---\n",
    "            cost = criterion(preds.permute(1, 0, 2), text_for_loss.to(device), preds_size.to(device), length_for_loss.to(device))\n",
    "            # -----------------------------------------------------------------\n",
    "\n",
    "            if opt.decode == 'greedy':\n",
    "                # Select max probabilty (greedy decoding) then decode index to character\n",
    "                _, preds_index = preds.max(2)\n",
    "                preds_index = preds_index.view(-1)\n",
    "                preds_str = converter.decode_greedy(preds_index.data, preds_size.data)\n",
    "            elif opt.decode == 'beamsearch':\n",
    "                preds_str = converter.decode_beamsearch(preds, beamWidth=2)\n",
    "\n",
    "        else:\n",
    "            preds = model(image, text_for_pred, is_train=False)\n",
    "            forward_time = time.time() - start_time\n",
    "\n",
    "            preds = preds[:, :text_for_loss.shape[1] - 1, :]\n",
    "            target = text_for_loss[:, 1:]  # without [GO] Symbol\n",
    "            cost = criterion(preds.contiguous().view(-1, preds.shape[-1]), target.contiguous().view(-1))\n",
    "\n",
    "            # select max probabilty (greedy decoding) then decode index to character\n",
    "            _, preds_index = preds.max(2)\n",
    "            preds_str = converter.decode(preds_index, length_for_pred)\n",
    "            labels = converter.decode(text_for_loss[:, 1:], length_for_loss)\n",
    "\n",
    "\n",
    "        infer_time += forward_time\n",
    "        valid_loss_avg.add(cost)\n",
    "\n",
    "        # calculate accuracy & norm ED\n",
    "        preds_prob = F.softmax(preds, dim=2)\n",
    "        preds_max_prob, _ = preds_prob.max(dim=2)\n",
    "        confidence_score_list = []\n",
    "        \n",
    "        for gt, pred, pred_max_prob in zip(labels, preds_str, preds_max_prob):\n",
    "            if 'Attn' in opt.Prediction:\n",
    "                gt = gt[:gt.find('[s]')]\n",
    "                pred_EOS = pred.find('[s]')\n",
    "                pred = pred[:pred_EOS]  # prune after \"end of sentence\" token ([s])\n",
    "                pred_max_prob = pred_max_prob[:pred_EOS]\n",
    "\n",
    "            if pred == gt:\n",
    "                n_correct += 1\n",
    "\n",
    "            '''\n",
    "            (old version) ICDAR2017 DOST Normalized Edit Distance https://rrc.cvc.uab.es/?ch=7&com=tasks\n",
    "            \"For each word we calculate the normalized edit distance to the length of the ground truth transcription.\" \n",
    "            if len(gt) == 0:\n",
    "                norm_ED += 1\n",
    "            else:\n",
    "                norm_ED += edit_distance(pred, gt) / len(gt)\n",
    "            '''\n",
    "            \n",
    "            # ICDAR2019 Normalized Edit Distance \n",
    "            if len(gt) == 0 or len(pred) ==0:\n",
    "                norm_ED += 0\n",
    "            elif len(gt) > len(pred):\n",
    "                norm_ED += 1 - edit_distance(pred, gt) / len(gt)\n",
    "            else:\n",
    "                norm_ED += 1 - edit_distance(pred, gt) / len(pred)\n",
    "\n",
    "            # calculate confidence score (= multiply of pred_max_prob)\n",
    "            try:\n",
    "                confidence_score = pred_max_prob.cumprod(dim=0)[-1]\n",
    "            except:\n",
    "                confidence_score = 0  # for empty pred case, when prune after \"end of sentence\" token ([s])\n",
    "            confidence_score_list.append(confidence_score)\n",
    "            # print(pred, gt, pred==gt, confidence_score)\n",
    "\n",
    "    accuracy = n_correct / float(length_of_data) * 100\n",
    "    norm_ED = norm_ED / float(length_of_data) # ICDAR2019 Normalized Edit Distance\n",
    "\n",
    "    return valid_loss_avg.val(), accuracy, norm_ED, preds_str, confidence_score_list, labels, infer_time, length_of_data\n",
    "\n",
    "# The rest of the file can be included if you plan to run test.py directly\n",
    "# but for the purpose of fixing the training loop, this is sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T01:30:51.466825Z",
     "iopub.status.busy": "2025-06-25T01:30:51.466554Z",
     "iopub.status.idle": "2025-06-25T01:30:51.500455Z",
     "shell.execute_reply": "2025-06-25T01:30:51.499713Z",
     "shell.execute_reply.started": "2025-06-25T01:30:51.466804Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./dataset.py\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import six\n",
    "import math\n",
    "import torch\n",
    "import pandas  as pd\n",
    "\n",
    "from natsort import natsorted\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, ConcatDataset, Subset\n",
    "from torch._utils import _accumulate\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def contrast_grey(img):\n",
    "    high = np.percentile(img, 90)\n",
    "    low  = np.percentile(img, 10)\n",
    "    return (high-low)/(high+low), high, low\n",
    "\n",
    "def adjust_contrast_grey(img, target = 0.4):\n",
    "    contrast, high, low = contrast_grey(img)\n",
    "    if contrast < target:\n",
    "        img = img.astype(int)\n",
    "        ratio = 200./(high-low)\n",
    "        img = (img - low + 25)*ratio\n",
    "        img = np.maximum(np.full(img.shape, 0) ,np.minimum(np.full(img.shape, 255), img)).astype(np.uint8)\n",
    "    return img\n",
    "\n",
    "\n",
    "class Batch_Balanced_Dataset(object):\n",
    "\n",
    "    def __init__(self, opt):\n",
    "        \"\"\"\n",
    "        Modulate the data ratio in the batch.\n",
    "        For example, when select_data is \"MJ-ST\" and batch_ratio is \"0.5-0.5\",\n",
    "        the 50% of the batch is filled with MJ and the other 50% of the batch is filled with ST.\n",
    "        \"\"\"\n",
    "        log = open(f'./saved_models/{opt.experiment_name}/log_dataset.txt', 'a')\n",
    "        dashed_line = '-' * 80\n",
    "        print(dashed_line)\n",
    "        log.write(dashed_line + '\\n')\n",
    "        print(f'dataset_root: {opt.train_data}\\nopt.select_data: {opt.select_data}\\nopt.batch_ratio: {opt.batch_ratio}')\n",
    "        log.write(f'dataset_root: {opt.train_data}\\nopt.select_data: {opt.select_data}\\nopt.batch_ratio: {opt.batch_ratio}\\n')\n",
    "        assert len(opt.select_data) == len(opt.batch_ratio)\n",
    "\n",
    "        _AlignCollate = AlignCollate(imgH=opt.imgH, imgW=opt.imgW, keep_ratio_with_pad=opt.PAD, contrast_adjust = opt.contrast_adjust)\n",
    "        self.data_loader_list = []\n",
    "        self.dataloader_iter_list = []\n",
    "        batch_size_list = []\n",
    "        Total_batch_size = 0\n",
    "        for selected_d, batch_ratio_d in zip(opt.select_data, opt.batch_ratio):\n",
    "            _batch_size = max(round(opt.batch_size * float(batch_ratio_d)), 1)\n",
    "            print(dashed_line)\n",
    "            log.write(dashed_line + '\\n')\n",
    "            _dataset, _dataset_log = hierarchical_dataset(root=opt.train_data, opt=opt, select_data=[selected_d])\n",
    "            total_number_dataset = len(_dataset)\n",
    "            log.write(_dataset_log)\n",
    "\n",
    "            \"\"\"\n",
    "            The total number of data can be modified with opt.total_data_usage_ratio.\n",
    "            ex) opt.total_data_usage_ratio = 1 indicates 100% usage, and 0.2 indicates 20% usage.\n",
    "            See 4.2 section in our paper.\n",
    "            \"\"\"\n",
    "            number_dataset = int(total_number_dataset * float(opt.total_data_usage_ratio))\n",
    "            dataset_split = [number_dataset, total_number_dataset - number_dataset]\n",
    "            indices = range(total_number_dataset)\n",
    "            _dataset, _ = [Subset(_dataset, indices[offset - length:offset])\n",
    "                           for offset, length in zip(_accumulate(dataset_split), dataset_split)]\n",
    "            selected_d_log = f'num total samples of {selected_d}: {total_number_dataset} x {opt.total_data_usage_ratio} (total_data_usage_ratio) = {len(_dataset)}\\n'\n",
    "            selected_d_log += f'num samples of {selected_d} per batch: {opt.batch_size} x {float(batch_ratio_d)} (batch_ratio) = {_batch_size}'\n",
    "            print(selected_d_log)\n",
    "            log.write(selected_d_log + '\\n')\n",
    "            batch_size_list.append(str(_batch_size))\n",
    "            Total_batch_size += _batch_size\n",
    "\n",
    "            _data_loader = torch.utils.data.DataLoader(\n",
    "                _dataset, batch_size=_batch_size,\n",
    "                shuffle=True,\n",
    "                num_workers=int(opt.workers), #prefetch_factor=2,persistent_workers=True,\n",
    "                collate_fn=_AlignCollate, pin_memory=True)\n",
    "            self.data_loader_list.append(_data_loader)\n",
    "            self.dataloader_iter_list.append(iter(_data_loader))\n",
    "\n",
    "        Total_batch_size_log = f'{dashed_line}\\n'\n",
    "        batch_size_sum = '+'.join(batch_size_list)\n",
    "        Total_batch_size_log += f'Total_batch_size: {batch_size_sum} = {Total_batch_size}\\n'\n",
    "        Total_batch_size_log += f'{dashed_line}'\n",
    "        opt.batch_size = Total_batch_size\n",
    "\n",
    "        print(Total_batch_size_log)\n",
    "        log.write(Total_batch_size_log + '\\n')\n",
    "        log.close()\n",
    "\n",
    "    def get_batch(self):\n",
    "        balanced_batch_images = []\n",
    "        balanced_batch_texts = []\n",
    "\n",
    "        for i, data_loader_iter in enumerate(self.dataloader_iter_list):\n",
    "            try:\n",
    "                image,text = next(iter(data_loader_iter))\n",
    "                balanced_batch_images.append(image)\n",
    "                balanced_batch_texts += text\n",
    "            except StopIteration:\n",
    "                self.dataloader_iter_list[i] = iter(self.data_loader_list[i])\n",
    "                image, text = next(iter(self.dataloader_iter_list[i]))\n",
    "                balanced_batch_images.append(image)\n",
    "                balanced_batch_texts += text\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "        balanced_batch_images = torch.cat(balanced_batch_images, 0)\n",
    "\n",
    "        return balanced_batch_images, balanced_batch_texts\n",
    "\n",
    "\n",
    "def hierarchical_dataset(root, opt, select_data='/'):\n",
    "    \"\"\" select_data='/' contains all sub-directory of root directory \"\"\"\n",
    "    dataset_list = []\n",
    "    dataset_log = f'dataset_root:    {root}\\t dataset: {select_data[0]}'\n",
    "    print(dataset_log)\n",
    "    dataset_log += '\\n'\n",
    "    for dirpath, dirnames, filenames in os.walk(root+'/'):\n",
    "        if not dirnames:\n",
    "            select_flag = False\n",
    "            for selected_d in select_data:\n",
    "                if selected_d in dirpath:\n",
    "                    select_flag = True\n",
    "                    break\n",
    "\n",
    "            if select_flag:\n",
    "                dataset = OCRDataset(dirpath, opt)\n",
    "                sub_dataset_log = f'sub-directory:\\t/{os.path.relpath(dirpath, root)}\\t num samples: {len(dataset)}'\n",
    "                print(sub_dataset_log)\n",
    "                dataset_log += f'{sub_dataset_log}\\n'\n",
    "                dataset_list.append(dataset)\n",
    "\n",
    "    concatenated_dataset = ConcatDataset(dataset_list)\n",
    "\n",
    "    return concatenated_dataset, dataset_log\n",
    "\n",
    "class OCRDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root, opt):\n",
    "\n",
    "        self.root = root\n",
    "        self.opt = opt\n",
    "        print(root)\n",
    "        self.df = pd.read_csv(os.path.join(root,'labels.csv'), engine='python', usecols=['filename', 'words'], keep_default_na=False)\n",
    "        self.nSamples = len(self.df)\n",
    "\n",
    "        if self.opt.data_filtering_off:\n",
    "            self.filtered_index_list = [index + 1 for index in range(self.nSamples)]\n",
    "        else:\n",
    "            self.filtered_index_list = []\n",
    "            for index in range(self.nSamples):\n",
    "                label = self.df.at[index,'words']\n",
    "                try:\n",
    "                    if len(label) > self.opt.batch_max_length:\n",
    "                        continue\n",
    "                except:\n",
    "                    print(label)\n",
    "                out_of_char = f'[^{self.opt.character}]'\n",
    "                if re.search(out_of_char, label.lower()):\n",
    "                    continue\n",
    "                self.filtered_index_list.append(index)\n",
    "            self.nSamples = len(self.filtered_index_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.nSamples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = self.filtered_index_list[index]\n",
    "        img_fname = self.df.at[index,'filename']\n",
    "        img_fpath = os.path.join(self.root, img_fname)\n",
    "        label = self.df.at[index,'words']\n",
    "\n",
    "        if self.opt.rgb:\n",
    "            img = Image.open(img_fpath).convert('RGB')  # for color image\n",
    "        else:\n",
    "            img = Image.open(img_fpath).convert('L')\n",
    "\n",
    "        if not self.opt.sensitive:\n",
    "            label = label.lower()\n",
    "\n",
    "        # We only train and evaluate on alphanumerics (or pre-defined character set in train.py)\n",
    "        out_of_char = f'[^{self.opt.character}]'\n",
    "        label = re.sub(out_of_char, '', label)\n",
    "\n",
    "        return (img, label)\n",
    "\n",
    "class ResizeNormalize(object):\n",
    "\n",
    "    def __init__(self, size, interpolation=Image.BICUBIC):\n",
    "        self.size = size\n",
    "        self.interpolation = interpolation\n",
    "        self.toTensor = transforms.ToTensor()\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = img.resize(self.size, self.interpolation)\n",
    "        img = self.toTensor(img)\n",
    "        img.sub_(0.5).div_(0.5)\n",
    "        return img\n",
    "\n",
    "\n",
    "class NormalizePAD(object):\n",
    "\n",
    "    def __init__(self, max_size, PAD_type='right'):\n",
    "        self.toTensor = transforms.ToTensor()\n",
    "        self.max_size = max_size\n",
    "        self.max_width_half = math.floor(max_size[2] / 2)\n",
    "        self.PAD_type = PAD_type\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = self.toTensor(img)\n",
    "        img.sub_(0.5).div_(0.5)\n",
    "        c, h, w = img.size()\n",
    "        Pad_img = torch.FloatTensor(*self.max_size).fill_(0)\n",
    "        Pad_img[:, :, :w] = img  # right pad\n",
    "        if self.max_size[2] != w:  # add border Pad\n",
    "            Pad_img[:, :, w:] = img[:, :, w - 1].unsqueeze(2).expand(c, h, self.max_size[2] - w)\n",
    "\n",
    "        return Pad_img\n",
    "\n",
    "\n",
    "class AlignCollate(object):\n",
    "\n",
    "    def __init__(self, imgH=32, imgW=100, keep_ratio_with_pad=False, contrast_adjust = 0.):\n",
    "        self.imgH = imgH\n",
    "        self.imgW = imgW\n",
    "        self.keep_ratio_with_pad = keep_ratio_with_pad\n",
    "        self.contrast_adjust = contrast_adjust\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        batch = filter(lambda x: x is not None, batch)\n",
    "        images, labels = zip(*batch)\n",
    "\n",
    "        if self.keep_ratio_with_pad:  # same concept with 'Rosetta' paper\n",
    "            resized_max_w = self.imgW\n",
    "            input_channel = 3 if images[0].mode == 'RGB' else 1\n",
    "            transform = NormalizePAD((input_channel, self.imgH, resized_max_w))\n",
    "\n",
    "            resized_images = []\n",
    "            for image in images:\n",
    "                w, h = image.size\n",
    "\n",
    "                #### augmentation here - change contrast\n",
    "                if self.contrast_adjust > 0:\n",
    "                    image = np.array(image.convert(\"L\"))\n",
    "                    image = adjust_contrast_grey(image, target = self.contrast_adjust)\n",
    "                    image = Image.fromarray(image, 'L')\n",
    "\n",
    "                ratio = w / float(h)\n",
    "                if math.ceil(self.imgH * ratio) > self.imgW:\n",
    "                    resized_w = self.imgW\n",
    "                else:\n",
    "                    resized_w = math.ceil(self.imgH * ratio)\n",
    "\n",
    "                resized_image = image.resize((resized_w, self.imgH), Image.BICUBIC)\n",
    "                resized_images.append(transform(resized_image))\n",
    "                # resized_image.save('./image_test/%d_test.jpg' % w)\n",
    "\n",
    "            image_tensors = torch.cat([t.unsqueeze(0) for t in resized_images], 0)\n",
    "\n",
    "        else:\n",
    "            transform = ResizeNormalize((self.imgW, self.imgH))\n",
    "            image_tensors = [transform(image) for image in images]\n",
    "            image_tensors = torch.cat([t.unsqueeze(0) for t in image_tensors], 0)\n",
    "\n",
    "        return image_tensors, labels\n",
    "\n",
    "\n",
    "def tensor2im(image_tensor, imtype=np.uint8):\n",
    "    image_numpy = image_tensor.cpu().float().numpy()\n",
    "    if image_numpy.shape[0] == 1:\n",
    "        image_numpy = np.tile(image_numpy, (3, 1, 1))\n",
    "    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\n",
    "    return image_numpy.astype(imtype)\n",
    "\n",
    "\n",
    "def save_image(image_numpy, image_path):\n",
    "    image_pil = Image.fromarray(image_numpy)\n",
    "    image_pil.save(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patching file: dataset.py\n",
      "Patch complete. The file has been modified.\n",
      "\n",
      "You can now re-run your training command.\n"
     ]
    }
   ],
   "source": [
    "import fileinput\n",
    "import sys\n",
    "\n",
    "# Define the path to the problematic file\n",
    "file_path = 'dataset.py'\n",
    "\n",
    "# The code for the missing _accumulate function\n",
    "accumulate_code = \"\"\"\n",
    "import operator\n",
    "def _accumulate(iterable, fn=operator.add):\n",
    "    'Return running totals'\n",
    "    # _accumulate([1,2,3,4,5]) --> 1 3 6 10 15\n",
    "    # _accumulate([1,2,3,4,5], operator.mul) --> 1 2 6 24 120\n",
    "    it = iter(iterable)\n",
    "    try:\n",
    "        total = next(it)\n",
    "    except StopIteration:\n",
    "        return\n",
    "    yield total\n",
    "    for element in it:\n",
    "        total = fn(total, element)\n",
    "        yield total\n",
    "\"\"\"\n",
    "\n",
    "# Flag to check if we've added our code yet\n",
    "code_added = False\n",
    "\n",
    "print(f\"Patching file: {file_path}\")\n",
    "\n",
    "# Use fileinput to modify the file in-place\n",
    "for line in fileinput.input(file_path, inplace=True):\n",
    "    # --- Part 1: Comment out the broken import ---\n",
    "    if \"from torch._utils import _accumulate\" in line:\n",
    "        sys.stdout.write(\"# \" + line) # Comment out the line\n",
    "        continue # Skip to the next line\n",
    "\n",
    "    # --- Part 2: Add our function definition ---\n",
    "    # We add our code block right after the last import statement\n",
    "    if line.strip().startswith(\"import\") and not code_added:\n",
    "        sys.stdout.write(line) # Write the import line\n",
    "        # After writing the last import, add our custom function\n",
    "        sys.stdout.write(accumulate_code + \"\\n\")\n",
    "        code_added = True\n",
    "        continue\n",
    "        \n",
    "    # Write all other lines back to the file as they were\n",
    "    sys.stdout.write(line)\n",
    "\n",
    "print(\"Patch complete. The file has been modified.\")\n",
    "print(\"\\nYou can now re-run your training command.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting natsort\n",
      "  Downloading natsort-8.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Downloading natsort-8.4.0-py3-none-any.whl (38 kB)\n",
      "Installing collected packages: natsort\n",
      "Successfully installed natsort-8.4.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ! pip install natsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.7.24)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nltk\n",
      "Successfully installed nltk-3.9.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ! pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train EasyOCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.append('/kaggle/working/EasyOCR/')\n",
    "from utils import AttrDict\n",
    "from train import train\n",
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = True\n",
    "cudnn.deterministic = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T01:31:35.154228Z",
     "iopub.status.busy": "2025-06-25T01:31:35.153566Z",
     "iopub.status.idle": "2025-06-25T02:08:17.356404Z",
     "shell.execute_reply": "2025-06-25T02:08:17.354782Z",
     "shell.execute_reply.started": "2025-06-25T01:31:35.154200Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_config(file_path):\n",
    "    with open(file_path, 'r', encoding=\"utf8\") as stream:\n",
    "        opt = yaml.safe_load(stream)\n",
    "    opt = AttrDict(opt)\n",
    "    if opt.lang_char == 'None':\n",
    "        characters = ''\n",
    "        for data in opt['select_data'].split('-'):\n",
    "            csv_path = os.path.join(opt['train_data'], data, 'labels.csv')\n",
    "            df = pd.read_csv(csv_path, sep=';', engine='python', usecols=['filename', 'words'], keep_default_na=False)\n",
    "            all_char = ''.join(df['words'])\n",
    "            characters += ''.join(set(all_char))\n",
    "        characters = sorted(set(characters))\n",
    "        opt.character= ''.join(characters)\n",
    "    else:\n",
    "        opt.character = opt.number + opt.symbol + opt.lang_char\n",
    "    os.makedirs(f'./saved_models/{opt.experiment_name}', exist_ok=True)\n",
    "    return opt\n",
    "\n",
    "#Запускаем обучение\n",
    "opt = get_config(\"./config.yaml\")\n",
    "train(opt, amp=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EasyOCR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T02:11:36.833176Z",
     "iopub.status.busy": "2025-06-25T02:11:36.832899Z",
     "iopub.status.idle": "2025-06-25T02:11:37.095438Z",
     "shell.execute_reply": "2025-06-25T02:11:37.094358Z",
     "shell.execute_reply.started": "2025-06-25T02:11:36.833157Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.makedirs('./models/easyocr_finetuned/', exist_ok=True)\n",
    "! cp ./best_accuracy.pth ./models/easyocr_finetuned/best_accuracy.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "! cp config.yaml ./models/easyocr_finetuned/best_accuracy.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T04:40:37.504565Z",
     "iopub.status.busy": "2025-06-25T04:40:37.503751Z",
     "iopub.status.idle": "2025-06-25T04:40:37.510467Z",
     "shell.execute_reply": "2025-06-25T04:40:37.509684Z",
     "shell.execute_reply.started": "2025-06-25T04:40:37.504540Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import easyocr\n",
    "import numpy as np\n",
    "\n",
    "class EasyOCREvaluator:\n",
    "    def __init__(self, model_path='models/easyocr_finetuned/'):\n",
    "        \"\"\"\n",
    "        Initializes the EasyOCR reader with our fine-tuned recognition model.\n",
    "        \"\"\"\n",
    "        print(\"Initializing EasyOCR with fine-tuned model...\")\n",
    "        # We use the standard detector but specify our own recognizer\n",
    "        self.reader = easyocr.Reader(lang_list=['ru', 'en'], # Languages\n",
    "            gpu=True,\n",
    "            model_storage_directory=model_path,\n",
    "            user_network_directory=model_path,\n",
    "            recog_network='best_accuracy' # Tells EasyOCR to look for a custom model\n",
    "        )\n",
    "\n",
    "    def predict(self, image_path: str):\n",
    "        \"\"\"\n",
    "        Takes an image path and returns structured predictions.\n",
    "        \"\"\"\n",
    "        # EasyOCR's readtext performs both detection and recognition\n",
    "        results = self.reader.readtext(image_path)\n",
    "        \n",
    "        predictions = []\n",
    "        for (bbox, text, prob) in results:\n",
    "            # bbox is [[x1,y1], [x2,y1], [x2,y2], [x1,y2]]\n",
    "            # Convert it to the 8-point format our evaluator expects\n",
    "            points = np.array(bbox).flatten().tolist()\n",
    "            predictions.append({\n",
    "                'text': text,\n",
    "                'points': points\n",
    "            })\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T03:26:32.760111Z",
     "iopub.status.busy": "2025-06-25T03:26:32.759284Z",
     "iopub.status.idle": "2025-06-25T03:26:32.763677Z",
     "shell.execute_reply": "2025-06-25T03:26:32.762942Z",
     "shell.execute_reply.started": "2025-06-25T03:26:32.760088Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T03:26:26.806236Z",
     "iopub.status.busy": "2025-06-25T03:26:26.805627Z",
     "iopub.status.idle": "2025-06-25T03:26:26.813316Z",
     "shell.execute_reply": "2025-06-25T03:26:26.812512Z",
     "shell.execute_reply.started": "2025-06-25T03:26:26.806209Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def run_cer_evaluation(evaluator, filepaths, batch_size=8):\n",
    "    \"\"\"\n",
    "    Runs a full evaluation loop for a given model and calculates the average CER.\n",
    "    \"\"\"\n",
    "    all_cer_scores = []\n",
    "    \n",
    "    # Determine if the evaluator supports batching\n",
    "    supports_batching = hasattr(evaluator, 'predict_batch')\n",
    "\n",
    "    if supports_batching:\n",
    "        # Create batches of filepaths for batched evaluators (like Qwen)\n",
    "        filepath_batches = [filepaths[i:i + batch_size] for i in range(0, len(filepaths), batch_size)]\n",
    "        \n",
    "        for batch in tqdm(filepath_batches, desc=f\"Evaluating {evaluator.__class__.__name__} in batches\"):\n",
    "            image_paths_batch = [fp[0] for fp in batch]\n",
    "            \n",
    "            batch_predictions = evaluator.predict_batch(image_paths_batch)\n",
    "            \n",
    "            for i, predictions_for_one_image in enumerate(batch_predictions):\n",
    "                img_path, gt_path = batch[i]\n",
    "                _, annotations = load_sample(img_path, gt_path)\n",
    "                if not annotations: continue\n",
    "                \n",
    "                image_cer = calculate_image_cer(predictions_for_one_image, annotations)\n",
    "                all_cer_scores.append(image_cer)\n",
    "    else:\n",
    "        # Process one by one for non-batched evaluators (like EasyOCR)\n",
    "        for img_path, gt_path in tqdm(filepaths, desc=f\"Evaluating {evaluator.__class__.__name__}\"):\n",
    "            image, annotations = load_sample(img_path, gt_path)\n",
    "            if not annotations: continue\n",
    "            if hasattr(evaluator, 'inference'):\n",
    "                # prompt = prompt ='Read all the text in the image. For each section of text, print its bounding box and text in this bounding box in the format: {(x1,y1),(x2,y2)} -- text. where x1'\n",
    "                # predictions = evaluator.predict(img_path[0])\n",
    "                predictions = evaluator.inference(image, img_path)\n",
    "            else:\n",
    "                predictions = evaluator.predict(img_path)\n",
    "                \n",
    "            image_cer = calculate_image_cer(predictions, annotations)\n",
    "            all_cer_scores.append(image_cer)\n",
    "            del image\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "    # Calculate the final average CER across the whole dataset\n",
    "    average_cer = sum(all_cer_scores) / len(all_cer_scores) if all_cer_scores else 0.0\n",
    "    return average_cer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T04:40:42.778624Z",
     "iopub.status.busy": "2025-06-25T04:40:42.778076Z",
     "iopub.status.idle": "2025-06-25T04:40:54.716534Z",
     "shell.execute_reply": "2025-06-25T04:40:54.715829Z",
     "shell.execute_reply.started": "2025-06-25T04:40:42.778598Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "filepaths = get_all_filepaths(path_test, groundtruth_test_path)\n",
    "filepaths = filepaths[:50] # Uncomment for a quick test run\n",
    "\n",
    "# --- Qwen-VL Evaluation ---\n",
    "print(\"--- Evaluating Qwen-VL (VLM Baseline) ---\")\n",
    "qwen_evaluator = QwenEvaluator(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n",
    "qwen_avg_cer = run_cer_evaluation(qwen_evaluator, filepaths, batch_size=8)\n",
    "\n",
    "# --- EasyOCR Fine-Tuned Evaluation ---\n",
    "print(\"\\n--- Evaluating EasyOCR (Fine-Tuned Pipeline) ---\")\n",
    "easyocr_evaluator = EasyOCREvaluator(model_path='models/easyocr_finetuned/')\n",
    "easyocr_avg_cer = run_cer_evaluation(easyocr_evaluator, filepaths)\n",
    "\n",
    "# --- Final Comparison Report ---\n",
    "print(\"\\n--- Final Comparison Report (Average Character Error Rate) ---\")\n",
    "print(\"Lower is better.\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Model':<30} | {'Average CER':<15}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Qwen-VL (Zero-Shot VLM)':<30} | {qwen_avg_cer:<15.4f}\")\n",
    "print(f\"{'EasyOCR (Fine-Tuned)':<30} | {easyocr_avg_cer:<15.4f}\")\n",
    "print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5176558,
     "sourceId": 8644564,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 159219517,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
